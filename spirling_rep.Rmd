<<<<<<< Updated upstream
<<<<<<< Updated upstream
---
title: "Replication Exercise 1"
author: "Sam Cohen & Priscila Stisman"
date: "2025-01-21"
output: html_document
---
# Setup

```{r}
library(tidyverse)
library(dplyr)
library(pacman)
library(quanteda)
library(pdftools)
library(broom)
library(here)
pacman::p_load(readtext, quanteda.textstats)

set.seed(10) # Setting sed for replication purposes

```

## Reading in metadata

```{r}
# Reading in data
bigframe <- get(load("bigframe.RData")) 
bigframe %>% head()

```

## Reading in speeches
```{r}
# Fetching all speech csvs
speech_csvs <- list.files("speeches_by_parliament", pattern = "^speech.*.csv$", full.names = TRUE) # choosing csvs that are speeches 

# it starts with speech
# * has something in the middle
# $ ends with

# Combining csv
speeches <- speech_csvs %>%
  lapply(read_csv) %>%  #lapply apply the read_csv function - lappy applies a function (that already exists in r) to a set of dfs
  bind_rows() #stack the csv files

```

# Cleaning

## Cleaning bigframe data
```{r}
# Cleaning DF
bigframe$year <- substr(bigframe$year.dummy, 1, 4) %>% as.integer() # Creating year variable #taking the first 4 characters

```

## Preprocessing corpus
```{r}
# Cleaning
speeches$year <- format(speeches$date, "%Y") # creating year variable
speech_samples <- speeches %>% filter(year >= 1832 & year <= 1915) # filtering for correct years
speech_samples <- speeches %>% sample_n(10000) # Taking  n=10000 sample of speeches

# Creating corpus
corpus <- corpus(speech_samples$body)

#ccorpus2 <- corpus(speech_samples, text_field = "body") - other way of doing it.

# Preprocessing 
tokens <- tokens(corpus, 
                 split_hyphens = FALSE, # keep hyphenated words
                 to_lower = TRUE, # convert all to lowercase 
                 remove_punct = TRUE, # remove punctuation
                 remove_numbers = TRUE, # remove digits
                 remove_symbols = TRUE, # remove symbols
                 valuetype = "regex") %>%  
  tokens_tolower() %>% # to lowercase
  tokens_remove(stopwords("en")) # remove stopwords
tokens <- tokens_wordstem(tokens) # stemming (removing the end of words)

```
```{r}
# Freeing up space for speed/efficiency
rm(speeches, speech_csvs, big.frame)
gc() 
```

# Text Analysis

## DFM
```{r}
# DFM
dfm_tokens <- dfm(tokens, min_docfreq = 0.05, max_docfreq = 0.95, docfreq_type = "prop", verbose = TRUE)

#take out the very rare/very common tokens that don't help discriminate among docs
```

## TF-IDF
```{r}
# TF-IDF Matrix
tfidf_speeches <- dfm_tokens %>%
  dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax") 

# Top TFIDF featyres
topfeatures(tfidf_speeches)

```

## FRE
```{r}
# Creating DF for FRE replication for speeches
fre_scores <- textstat_readability(
  corpus(speech_samples$body),
  measure = "Flesch",
  remove_hyphens = FALSE,
  min_sentence_length = 1,
  max_sentence_length = 10000,
  intermediate = FALSE
)

# Calculating average FRE score for corpus
fre_scores$Flesch %>% mean(., na.rm=TRUE)

# Calculating SD of FRE score for corpus
fre_scores$Flesch %>% sd(., na.rm=TRUE)

# FRE summary
summary(fre_scores$Flesch)

# Adding calculated FRE scores to speech data sample
speech_samples <- cbind(speech_samples, fre_scores)

```

Note: -300 Flesch score.


# Visuals

## Avg. Readability over time by Cabinet position (replication) (p. 128)
```{r}
#
avg_readease_cabinet <- bigframe %>% group_by(year, cabinet) %>% summarize(avg_ease = mean(FK_read.ease))

# Over time
ggplot(avg_readease_cabinet, aes(x = year, y = avg_ease, color = cabinet)) +
  geom_line() +
  scale_color_manual(values = c('1' = "blue", '0' = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") 
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```
## OLS: Readability over time by Cabinet position (replication) (p. 128)
```{r}
# OLS
ggplot(avg_readease_cabinet, aes(x = year, y = avg_ease, color = cabinet)) +
  geom_smooth(method = "lm", se = TRUE) +
  scale_color_manual(values = c('1' = "blue", '0' = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal() 

# which are the regressors here in this regression?
```

## Boxplot: FRE scores (replication) (p. 127)
```{r}
ggplot(data=bigframe, mapping=aes(y=FK_read.ease))+geom_boxplot()

```
## Avg. Syllable per word count over time by cabinet position (replication) (p. 129)
```{r}
# Plotting
bigframe %>% group_by(year, cabinet) %>% 
  mutate(syl_per_word = syllable.count
/ word.count) %>% 
  summarize(mean_syl_per_word = mean(syl_per_word)) %>% 
  ggplot(., aes(x = year, y = mean_syl_per_word, color = cabinet)) + 
  geom_line() + 
  labs(y = "Syllable per Word Count", x = "Year") 

```
# Tests

## Regression: effect of cabinet membership on comprehensibility, with controls (replication) (p. 127 & 132)
```{r}
bigframe$post_reform <- ifelse(bigframe$year>=1868, 1, 0 ) # dummy for post reform 
lm(FK_read.ease ~ cabinet + post_reform + cabinet*post_reform + party + competitiveness + word.count, data=bigframe) %>% summary() %>% tidy()# regression showing effect of cabinet position on comprehensibility, with controls and interaction between cabinet position and post-reform dummy

```

# Original additions

## Avg. Readability over time by party affiliation (original)
```{r}
# Creating DF for mean readability by party and year 
avg_readease <- bigframe %>% group_by(year, party) %>% summarize(avg_ease = mean(FK_read.ease))

# Plotting
ggplot(avg_readease, aes(x = year, y = avg_ease, color = party)) +
  geom_line() +
  scale_color_manual(values = c("C" = "blue", "L" = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```

## Cosine similarity of texts (using TF-IDF score matrix)
```{r}

# Getting cosine similarity sample speeches using TFIDF scores
simil_speeches<- textstat_simil(tfidf_speeches, margin = "documents", method = "cosine")

# Creating matrix of speech cosine similarity scores
tfidf_sim <- as_tibble(as.matrix(simil_speeches)) %>%
  mutate(names = rownames(as.matrix(simil_speeches))) %>%
  select(names, everything()) %>% as.data.frame()

# Pivoting and selecting top 10 text similarity scores
library('reshape2')
tfidf_long <- melt(tfidf_sim, varnames = c("textA", "textB"), value.name = "cosine") # Reshaping matrix for diads

tfidf_long %>% arrange(desc(cosine)) %>% filter(cosine < 0.999) %>% head(10) # top 10 cosine similarity scores (>1 to account for identical text simil.)

```


```{r}
# Ex: 2nd closest cosine similarity scores for speeches
speech_samples[545,]
speech_samples[4161,]

```
=======
---
title: "Replication Code #1"
author: "Sam Cohen and Priscila Stisman"
date: "2025-01-21"
output: html_document
---
# Setup

```{r}
library(tidyverse)
library(dplyr)
library(pacman)
library(quanteda)
library(pdftools)
library(broom)
library(zoo)

pacman::p_load(readtext, quanteda.textstats)

```

## Reading in metadata

```{r}
# Reading in data
bigframe <- get(load("bigframe.RData")) # change dir
bigframe %>% head()

```

## Reading in speeches
```{r}
# Fetching all speech csvs
setwd("C:\\Users\\samue\\OneDrive\\Documents\\Grad school - Georgetown MS DSPP\\Second Year - Semester 2\\Text as Data\\Replication Projects\\1")
speech_csvs <- list.files("speeches_by_parliament", pattern = "^speech.*.csv$", full.names = TRUE) # choosing csvs that are speeches

# Combning csv
speeches <- speech_csvs %>%
  lapply(read_csv) %>%  
  bind_rows()

```

# Cleaning

## Cleaning bigframe data
```{r}
# Cleaning DF
bigframe$year <- substr(bigframe$year.dummy, 1, 4) %>% as.integer() # Creating year variable

bigframe <- bigframe %>% mutate(
  year.dummy = as.character(year.dummy),
  year_q = as.Date(as.yearqtr(year.dummy, format = "%Y_%q")) # Convert factor to character
  )


```

## Preprocessing corpus
```{r}
# Cleaning
speeches$year <- format(speeches$date, "%Y") # creating year variable
speech_samples <- speeches %>% filter(year >= 1832 & year <= 1915) # filtering for correct years
speech_samples <- speeches %>% sample_n(10000) # Taking  n=10000 sample of speeches

# Creating corpus
corpus <- corpus(speech_samples$body)

# Preprocessing 
tokens <- tokens(corpus, 
                 split_hyphens = FALSE, # keep hyphenated words
                 to_lower = TRUE, # convert all to lowercase 
                 remove_punct = TRUE, # remove punctuation
                 remove_numbers = TRUE, # remove digits
                 remove_symbols = TRUE, # remove symbols
                 valuetype = "regex") %>%  
  tokens_tolower() %>% # to lowercase
  tokens_remove(stopwords("en")) # remove stopwords
tokens <- tokens_wordstem(tokens) # stemming

```
```{r}
# Freeing up space for speed/efficiency
rm(speeches, speech_csvs, big.frame)
gc()
```

# Text Analysis

## DFM
```{r}
# DFM
dfm_tokens <- dfm(tokens, min_docfreq = 0.05, max_docfreq = 0.95, docfreq_type = "prop", verbose = TRUE)

```

## TF-IDF
```{r}
# TF-IDF Matrix
tfidf_speeches <- dfm_tokens %>%
  dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")

# Top TFIDF features
topfeatures(tfidf_speeches)

```

## FRE
```{r}
# Creating DF for FRE replication for speeches
fre_scores <- textstat_readability(
  corpus(speech_samples$body),
  measure = "Flesch",
  remove_hyphens = FALSE,
  min_sentence_length = 1,
  max_sentence_length = 10000,
  intermediate = FALSE
)

# Calculating average FRE score for corpus
fre_scores$Flesch %>% mean(., na.rm=TRUE)

# Calculating SD of FRE score for corpus
fre_scores$Flesch %>% sd(., na.rm=TRUE)

# FRE summary
summary(fre_scores$Flesch)

# Adding calculated FRE scores to speech data sample
speech_samples <- cbind(speech_samples, fre_scores)

```



# Visuals

## Avg. Readability over time by Cabinet position (replication) (p. 128)
```{r}
#
avg_readease_cabinet <- bigframe %>% group_by(year, cabinet) %>% summarize(avg_ease = mean(FK_read.ease))

# Over time
ggplot(avg_readease_cabinet, aes(x = year, y = avg_ease, color = cabinet)) +
  geom_line() +
  scale_color_manual(values = c('1' = "blue", '0' = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```
## OLS: Readability over time by Cabinet position (replication) (p. 128)
```{r}
# OLS
ggplot(avg_readease_cabinet, aes(x = year, y = avg_ease, color = cabinet)) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_manual(values = c('1' = "blue", '0' = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```

## Boxplot: FRE scores (replication) (p. 127)
```{r}
ggplot(data=bigframe, mapping=aes(y=FK_read.ease))+geom_boxplot()

```
## Avg. Syllable per word count over time by cabinet position (replication) (p. 129)
```{r}
# Plotting
bigframe %>% group_by(year_q, cabinet) %>% 
  mutate(syl_per_word = syllable.count
/ word.count) %>% 
  summarize(mean_syl_per_word = mean(syl_per_word)) %>% 
  ggplot(., aes(x = year_q, y = mean_syl_per_word, color = cabinet)) + 
  geom_line() + 
  labs(y = "Syllable per Word Count", x = "Year") 

```
# Tests

## Regression: effect of cabinet membership on comprehensibility, with controls (replication) (p. 127 & 132)
```{r}
bigframe$post_reform <- ifelse(bigframe$year>=1868, 1, 0 ) # dummy for post reform 
lm(FK_read.ease ~ cabinet + post_reform + cabinet*post_reform + party + competitiveness + word.count, data=bigframe) %>% summary() %>% tidy()# regression showing effect of cabinet position on comprehensibility, with controls and interaction between cabinet position and post-reform dummy

```

# Original additions

## Avg. Readability over time by party affiliation (original)
```{r}
# Creating DF for mean readability by party and year 
avg_readease <- bigframe %>% group_by(year, party) %>% summarize(avg_ease = mean(FK_read.ease))

# Plotting
ggplot(avg_readease, aes(x = year, y = avg_ease, color = party)) +
  geom_line() +
  scale_color_manual(values = c("C" = "blue", "L" = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```

## Cosine similarity of texts (using TF-IDF score matrix)
```{r}

# Getting cosine similarity sample speeches using TFIDF scores
simil_speeches<- textstat_simil(tfidf_speeches, margin = "documents", method = "cosine")

# Creating matrix of speech cosine similarity scores
tfidf_sim <- as_tibble(as.matrix(simil_speeches)) %>%
  mutate(names = rownames(as.matrix(simil_speeches))) %>%
  select(names, everything()) %>% as.data.frame()

# Pivoting and selecting top 10 text similarity scores
library('reshape2')
tfidf_long <- melt(tfidf_sim, varnames = c("textA", "textB"), value.name = "cosine") # Reshaping matrix for diads

tfidf_long %>% arrange(desc(cosine)) %>% filter(cosine < 0.999) %>% head(10) # top 10 cosine similarity scores (>1 to account for identical text simil.)

```


```{r}
# Ex: 2nd closest cosine similarity scores for speeches
speech_samples[545,]
speech_samples[4161,]

```

>>>>>>> Stashed changes
=======
---
title: "Replication Code #1"
author: "Sam Cohen and Priscila Stisman"
date: "2025-01-21"
output: html_document
---
# Setup

```{r}
library(tidyverse)
library(dplyr)
library(pacman)
library(quanteda)
library(pdftools)
library(broom)
library(zoo)

pacman::p_load(readtext, quanteda.textstats)

```

## Reading in metadata

```{r}
# Reading in data
bigframe <- get(load("bigframe.RData")) # change dir
bigframe %>% head()

```

## Reading in speeches
```{r}
# Fetching all speech csvs
setwd("C:\\Users\\samue\\OneDrive\\Documents\\Grad school - Georgetown MS DSPP\\Second Year - Semester 2\\Text as Data\\Replication Projects\\1")
speech_csvs <- list.files("speeches_by_parliament", pattern = "^speech.*.csv$", full.names = TRUE) # choosing csvs that are speeches

# Combning csv
speeches <- speech_csvs %>%
  lapply(read_csv) %>%  
  bind_rows()

```

# Cleaning

## Cleaning bigframe data
```{r}
# Cleaning DF
bigframe$year <- substr(bigframe$year.dummy, 1, 4) %>% as.integer() # Creating year variable

bigframe <- bigframe %>% mutate(
  year.dummy = as.character(year.dummy),
  year_q = as.Date(as.yearqtr(year.dummy, format = "%Y_%q")) # Convert factor to character
  )


```

## Preprocessing corpus
```{r}
# Cleaning
speeches$year <- format(speeches$date, "%Y") # creating year variable
speech_samples <- speeches %>% filter(year >= 1832 & year <= 1915) # filtering for correct years
speech_samples <- speeches %>% sample_n(10000) # Taking  n=10000 sample of speeches

# Creating corpus
corpus <- corpus(speech_samples$body)

# Preprocessing 
tokens <- tokens(corpus, 
                 split_hyphens = FALSE, # keep hyphenated words
                 to_lower = TRUE, # convert all to lowercase 
                 remove_punct = TRUE, # remove punctuation
                 remove_numbers = TRUE, # remove digits
                 remove_symbols = TRUE, # remove symbols
                 valuetype = "regex") %>%  
  tokens_tolower() %>% # to lowercase
  tokens_remove(stopwords("en")) # remove stopwords
tokens <- tokens_wordstem(tokens) # stemming

```
```{r}
# Freeing up space for speed/efficiency
rm(speeches, speech_csvs, big.frame)
gc()
```

# Text Analysis

## DFM
```{r}
# DFM
dfm_tokens <- dfm(tokens, min_docfreq = 0.05, max_docfreq = 0.95, docfreq_type = "prop", verbose = TRUE)

```

## TF-IDF
```{r}
# TF-IDF Matrix
tfidf_speeches <- dfm_tokens %>%
  dfm_tfidf(scheme_tf = "prop", scheme_df = "inversemax")

# Top TFIDF features
topfeatures(tfidf_speeches)

```

## FRE
```{r}
# Creating DF for FRE replication for speeches
fre_scores <- textstat_readability(
  corpus(speech_samples$body),
  measure = "Flesch",
  remove_hyphens = FALSE,
  min_sentence_length = 1,
  max_sentence_length = 10000,
  intermediate = FALSE
)

# Calculating average FRE score for corpus
fre_scores$Flesch %>% mean(., na.rm=TRUE)

# Calculating SD of FRE score for corpus
fre_scores$Flesch %>% sd(., na.rm=TRUE)

# FRE summary
summary(fre_scores$Flesch)

# Adding calculated FRE scores to speech data sample
speech_samples <- cbind(speech_samples, fre_scores)

```



# Visuals

## Avg. Readability over time by Cabinet position (replication) (p. 128)
```{r}
#
avg_readease_cabinet <- bigframe %>% group_by(year, cabinet) %>% summarize(avg_ease = mean(FK_read.ease))

# Over time
ggplot(avg_readease_cabinet, aes(x = year, y = avg_ease, color = cabinet)) +
  geom_line() +
  scale_color_manual(values = c('1' = "blue", '0' = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```
## OLS: Readability over time by Cabinet position (replication) (p. 128)
```{r}
# OLS
ggplot(avg_readease_cabinet, aes(x = year, y = avg_ease, color = cabinet)) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_manual(values = c('1' = "blue", '0' = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```

## Boxplot: FRE scores (replication) (p. 127)
```{r}
ggplot(data=bigframe, mapping=aes(y=FK_read.ease))+geom_boxplot()

```
## Avg. Syllable per word count over time by cabinet position (replication) (p. 129)
```{r}
# Plotting
bigframe %>% group_by(year_q, cabinet) %>% 
  mutate(syl_per_word = syllable.count
/ word.count) %>% 
  summarize(mean_syl_per_word = mean(syl_per_word)) %>% 
  ggplot(., aes(x = year_q, y = mean_syl_per_word, color = cabinet)) + 
  geom_line() + 
  labs(y = "Syllable per Word Count", x = "Year") 

```
# Tests

## Regression: effect of cabinet membership on comprehensibility, with controls (replication) (p. 127 & 132)
```{r}
bigframe$post_reform <- ifelse(bigframe$year>=1868, 1, 0 ) # dummy for post reform 
lm(FK_read.ease ~ cabinet + post_reform + cabinet*post_reform + party + competitiveness + word.count, data=bigframe) %>% summary() %>% tidy()# regression showing effect of cabinet position on comprehensibility, with controls and interaction between cabinet position and post-reform dummy

```

# Original additions

## Avg. Readability over time by party affiliation (original)
```{r}
# Creating DF for mean readability by party and year 
avg_readease <- bigframe %>% group_by(year, party) %>% summarize(avg_ease = mean(FK_read.ease))

# Plotting
ggplot(avg_readease, aes(x = year, y = avg_ease, color = party)) +
  geom_line() +
  scale_color_manual(values = c("C" = "blue", "L" = "red")) +  
  geom_vline(xintercept = 1867, linetype = "dashed", color = "darkgray") +
  labs(y = "FRE", x = "Year") + 
  theme_minimal()

```

## Cosine similarity of texts (using TF-IDF score matrix)
```{r}

# Getting cosine similarity sample speeches using TFIDF scores
simil_speeches<- textstat_simil(tfidf_speeches, margin = "documents", method = "cosine")

# Creating matrix of speech cosine similarity scores
tfidf_sim <- as_tibble(as.matrix(simil_speeches)) %>%
  mutate(names = rownames(as.matrix(simil_speeches))) %>%
  select(names, everything()) %>% as.data.frame()

# Pivoting and selecting top 10 text similarity scores
library('reshape2')
tfidf_long <- melt(tfidf_sim, varnames = c("textA", "textB"), value.name = "cosine") # Reshaping matrix for diads

tfidf_long %>% arrange(desc(cosine)) %>% filter(cosine < 0.999) %>% head(10) # top 10 cosine similarity scores (>1 to account for identical text simil.)

```


```{r}
# Ex: 1st closest cosine similarity scores for speeches
speech_samples[5846,]
speech_samples[1562,]

# EX: 2nd closest cosine similarity score for speeches
speech_samples[5158,]
speech_samples[1349,]

```
```{r}


```
>>>>>>> Stashed changes
